{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ef87be-5ef7-47dc-b7fb-1c923800e9ef",
   "metadata": {},
   "source": [
    "# 6. Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e4be9-ef39-4a9f-a107-78eb92ce6c3e",
   "metadata": {},
   "source": [
    "## Pre-requisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89705a3-6438-46b3-9019-168b1c0c7477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d4c39-ab67-4cf5-b511-3ea7cda98744",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3a8a23-8db3-4c58-baf9-81b34e161651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d205de-f5c4-4d24-86f7-c0c63714b494",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56542d3-b55a-44be-b4bf-b1b92f4006ba",
   "metadata": {},
   "source": [
    "하나의 epoch는 다음 두 부분으로 구성된다.\n",
    "- train loop - 학습용 dataset을 iterate하고 최적의 parameter로 수렴한다.\n",
    "- validation/test loop - model 성능이 개선되고 있는지 확인하기 위해 test datset을 iterate한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b8062-bd7f-4919-9007-099e7b3090e0",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7fbae-856b-499e-a6b9-93d8ad89267c",
   "metadata": {},
   "source": [
    "> 일반적인 loss function에는,  \n",
    "regression task에 사용하는 `nn.MSELoss`,  \n",
    "classification에 사용하는 `nn.NLLLoss`,  \n",
    "그리고 `nn.LogSoftmax`와 `nn.NLLLoss`를 합친 `nn.CrossEntropyLoss` 등이 있다.\n",
    "\n",
    "> model의 output logit을 `nn.CrossEntropyLoss`에 전달하여 logit을 정규화하고 예측 오류를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8a203c-3de2-4b1e-b253-a276b6180863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss function initializing\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19383b-3652-4264-9a8c-beb6b6eb7534",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f215d7-d7a9-4c32-a2f0-708e5058ccdf",
   "metadata": {},
   "source": [
    "> 모든 최적화 절차는 `optimizer` 개체에 캡슐화된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3da8636-9fee-4392-9199-b87af9c60331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer initializing\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac975fca-79db-4b3c-b90d-540a0b8b4ed6",
   "metadata": {},
   "source": [
    "train loop에서 최적화는 세 단계로 이루어진다.\n",
    "- `optimizer.zero_grad()`를 호출하여 model parameter의 gradient를 재설정한다.  \n",
    "기본적으로 gradient는 더해지기 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정한다.\n",
    "- `loss.backwards()`를 호출하여 prediction loss를 backpropagate한다.  \n",
    "pytorch는 각 parameter에 대해 loss의 gradient를 저장한다.\n",
    "- gradient를 계산한 뒤, `optimizer.step()`을 호출하여 backpropagate 단계에서 수집된 gradient로 parameter를 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001f699-e187-4a87-9438-c58792d3b2a7",
   "metadata": {},
   "source": [
    "## Full Implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aab91ce-adb3-4caa-a15f-466c8f195e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # prediction과 loss 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2acc6-e6fd-485b-94c8-ca3e8d0e2645",
   "metadata": {},
   "source": [
    "> loss function과 optimizer를 초기화하고, `train_loop`와 `test_loop`에 전달한다.  \n",
    "model의 성능 향상을 알아보기 위해 epoch수를 자유롭게 증가시켜볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c69084-d164-4cd0-8560-11eadac91a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------------\n",
      "loss: 2.302263 [   64/60000]\n",
      "loss: 2.291017 [ 6464/60000]\n",
      "loss: 2.274541 [12864/60000]\n",
      "loss: 2.272569 [19264/60000]\n",
      "loss: 2.242208 [25664/60000]\n",
      "loss: 2.220980 [32064/60000]\n",
      "loss: 2.225939 [38464/60000]\n",
      "loss: 2.191874 [44864/60000]\n",
      "loss: 2.192827 [51264/60000]\n",
      "loss: 2.153119 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 2.154046 \n",
      "\n",
      "Epoch 2\n",
      "---------------------------\n",
      "loss: 2.167067 [   64/60000]\n",
      "loss: 2.156435 [ 6464/60000]\n",
      "loss: 2.098273 [12864/60000]\n",
      "loss: 2.117744 [19264/60000]\n",
      "loss: 2.049263 [25664/60000]\n",
      "loss: 1.998094 [32064/60000]\n",
      "loss: 2.029153 [38464/60000]\n",
      "loss: 1.946053 [44864/60000]\n",
      "loss: 1.957492 [51264/60000]\n",
      "loss: 1.877311 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.875097 \n",
      "\n",
      "Epoch 3\n",
      "---------------------------\n",
      "loss: 1.917251 [   64/60000]\n",
      "loss: 1.883585 [ 6464/60000]\n",
      "loss: 1.761492 [12864/60000]\n",
      "loss: 1.805805 [19264/60000]\n",
      "loss: 1.683458 [25664/60000]\n",
      "loss: 1.642998 [32064/60000]\n",
      "loss: 1.676001 [38464/60000]\n",
      "loss: 1.574363 [44864/60000]\n",
      "loss: 1.603529 [51264/60000]\n",
      "loss: 1.497309 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.508575 \n",
      "\n",
      "Epoch 4\n",
      "---------------------------\n",
      "loss: 1.589373 [   64/60000]\n",
      "loss: 1.553054 [ 6464/60000]\n",
      "loss: 1.395690 [12864/60000]\n",
      "loss: 1.466092 [19264/60000]\n",
      "loss: 1.352238 [25664/60000]\n",
      "loss: 1.346984 [32064/60000]\n",
      "loss: 1.370322 [38464/60000]\n",
      "loss: 1.296570 [44864/60000]\n",
      "loss: 1.330637 [51264/60000]\n",
      "loss: 1.233499 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.250211 \n",
      "\n",
      "Epoch 5\n",
      "---------------------------\n",
      "loss: 1.340357 [   64/60000]\n",
      "loss: 1.323032 [ 6464/60000]\n",
      "loss: 1.149967 [12864/60000]\n",
      "loss: 1.250645 [19264/60000]\n",
      "loss: 1.134984 [25664/60000]\n",
      "loss: 1.154210 [32064/60000]\n",
      "loss: 1.181859 [38464/60000]\n",
      "loss: 1.123750 [44864/60000]\n",
      "loss: 1.160718 [51264/60000]\n",
      "loss: 1.078601 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.088920 \n",
      "\n",
      "Epoch 6\n",
      "---------------------------\n",
      "loss: 1.171991 [   64/60000]\n",
      "loss: 1.176631 [ 6464/60000]\n",
      "loss: 0.987427 [12864/60000]\n",
      "loss: 1.116395 [19264/60000]\n",
      "loss: 0.996893 [25664/60000]\n",
      "loss: 1.022683 [32064/60000]\n",
      "loss: 1.062945 [38464/60000]\n",
      "loss: 1.012010 [44864/60000]\n",
      "loss: 1.049494 [51264/60000]\n",
      "loss: 0.980456 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.983114 \n",
      "\n",
      "Epoch 7\n",
      "---------------------------\n",
      "loss: 1.053169 [   64/60000]\n",
      "loss: 1.080743 [ 6464/60000]\n",
      "loss: 0.874901 [12864/60000]\n",
      "loss: 1.026604 [19264/60000]\n",
      "loss: 0.906503 [25664/60000]\n",
      "loss: 0.927708 [32064/60000]\n",
      "loss: 0.983837 [38464/60000]\n",
      "loss: 0.937540 [44864/60000]\n",
      "loss: 0.971032 [51264/60000]\n",
      "loss: 0.913938 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.909465 \n",
      "\n",
      "Epoch 8\n",
      "---------------------------\n",
      "loss: 0.964615 [   64/60000]\n",
      "loss: 1.013332 [ 6464/60000]\n",
      "loss: 0.792886 [12864/60000]\n",
      "loss: 0.961951 [19264/60000]\n",
      "loss: 0.844311 [25664/60000]\n",
      "loss: 0.856446 [32064/60000]\n",
      "loss: 0.927275 [38464/60000]\n",
      "loss: 0.886257 [44864/60000]\n",
      "loss: 0.913302 [51264/60000]\n",
      "loss: 0.865496 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.855382 \n",
      "\n",
      "Epoch 9\n",
      "---------------------------\n",
      "loss: 0.896055 [   64/60000]\n",
      "loss: 0.962143 [ 6464/60000]\n",
      "loss: 0.730423 [12864/60000]\n",
      "loss: 0.912675 [19264/60000]\n",
      "loss: 0.799177 [25664/60000]\n",
      "loss: 0.801860 [32064/60000]\n",
      "loss: 0.884215 [38464/60000]\n",
      "loss: 0.849684 [44864/60000]\n",
      "loss: 0.869516 [51264/60000]\n",
      "loss: 0.827967 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.813892 \n",
      "\n",
      "Epoch 10\n",
      "---------------------------\n",
      "loss: 0.841299 [   64/60000]\n",
      "loss: 0.920653 [ 6464/60000]\n",
      "loss: 0.680999 [12864/60000]\n",
      "loss: 0.874033 [19264/60000]\n",
      "loss: 0.764624 [25664/60000]\n",
      "loss: 0.759465 [32064/60000]\n",
      "loss: 0.849720 [38464/60000]\n",
      "loss: 0.822494 [44864/60000]\n",
      "loss: 0.835271 [51264/60000]\n",
      "loss: 0.797406 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.780764 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n---------------------------')\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282d174-f13f-41f8-b5ec-4ed92ccc55bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
